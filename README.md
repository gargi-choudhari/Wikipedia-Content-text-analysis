# Wikipedia-Content-text-analysis
Wikipedia content text analysis. First, I web scrape the Wikipedia names using categories mentioned in the code file (Code in Final Listings(1).ipynb). The filtered list can be found in the CSV file in the repo (Final list.csv). The filtered list is used to get the Wikipedia content and pre-process it (Code in Processed Text data, ipynb). After this, I use the content to construct word embeddings and compute the WEAT scores for each Wikipedia page (code in Text Analysis.ipynb). In the same code file, you can find code for the Sentiment Analysis. Wordclouds for female and male positive and negative words were made(Code in Word Clouds for sentiments .ipynb). The code for logistic regression is present in a Stata do file(Logistic Regression Model.do). The data for the same is logit model data.xlsx file. Additionally, the average views per month per page were computed using the code in the average Views .ipynb file.
